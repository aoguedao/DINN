{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5b7282-ba88-4bfa-a5b5-7c82d0698d89",
   "metadata": {},
   "source": [
    "https://github.com/EbTech/AutoODE-DSL/blob/main/notebooks/Epidemic_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827e7855-1b49-4832-8dce-6a67406aaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import grad\n",
    "\n",
    "# plt.rc(\"text\", usetex=True)\n",
    "# plt.rc(\"font\", size=15, family=\"serif\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5abf21-c5b1-4688-b052-cbbe3a98d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SEVIRD:\n",
    "    L: float = 0.07143  # Natural birth rate\n",
    "    beta: float = 0.8  # Transmission rate \n",
    "    kappa: float = 0.21739  # Efficacy in preventing covid19 infection of vaccine\n",
    "    alpha: float = 0.10526  # Overal rate of vaccination\n",
    "    q: float = 0.0016  # Proportion vaccinated\n",
    "    sigma: float = 0.0255  # Fraction of exposed individuals moving to infected\n",
    "    gamma: float = 0.2  # Recovery rate\n",
    "    gamma_vax: float = 0.09901  # Recovery rate of individuals vaccinated\n",
    "    delta: float = 0.0255  # Death rate due to covid19\n",
    "    mu: float = 0.00553  # Natural death rate\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "    \n",
    "    @property\n",
    "    def names(self) -> List[str]:\n",
    "        return list(\"SEVIRD\")\n",
    "\n",
    "    def derivs(self, time: float, variables: np.ndarray):\n",
    "        S, E, V, I, R, D = variables\n",
    "        dS = self.L * self.N - self.beta * S * I / self.N - (self.mu + self.alpha) * S \n",
    "        dE = self.beta * I * (I + ( 1 - self.kappa) * V) / self.N - (self.sigma + self.mu) * E\n",
    "        dV = self.alpha * self.q * S - self.beta * (1 - self.kappa) * V * I / self.N - (self.gamma_vax + self.mu) *V\n",
    "        dI = self.sigma * E - (self.gamma + self.delta) * I - self.mu * I\n",
    "        dR = self.gamma * I + self.gamma_vax * V - self.mu * R\n",
    "        dD = self.delta * I\n",
    "        return np.array([dS, dE, dV, dI, dR, dD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f820d8df-566b-4875-aff3-ba04db97e710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x296f9e90b20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100.\n",
    "model = SEVIRD(N)\n",
    "start = [N * 0.99, N * 0.01, 0, 0, 0, 0]\n",
    "t_span = (0, 200)\n",
    "result = solve_ivp(model.derivs, t_span, start)\n",
    "\n",
    "t = result.t\n",
    "variables = result.y\n",
    "\n",
    "for v, name in zip(variables, model.names):\n",
    "    plt.plot(t, v, label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcde7b8-0808-4cfb-8e78-6913568bca7d",
   "metadata": {},
   "source": [
    "## DINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972d90f8-2a46-4063-b391-0dbade3945b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_data = variables[0, :]\n",
    "E_data = variables[1, :]\n",
    "V_data = variables[2, :]\n",
    "I_data = variables[3, :]\n",
    "R_data = variables[4, :]\n",
    "D_data = variables[5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df093-695e-4695-bc37-da2c4d9f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN(nn.Module):\n",
    "    def __init__(self, N, t, S_data, E_data, V_data, I_data, R_data, D_data):\n",
    "        super(DINN, self).__init__()\n",
    "        \n",
    "        self.N = N #population size\n",
    "        \n",
    "        #for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch\n",
    "        self.t = torch.tensor(t, requires_grad=True)\n",
    "        self.t_float = self.t.float()\n",
    "        self.t_batch = torch.reshape(self.t_float, (len(self.t), 1)) #reshape for batch \n",
    "\n",
    "        #for the compartments we just need to convert them into tensors\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.E = torch.tensor(E_data)\n",
    "        self.V = torch.tensor(V_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.R = torch.tensor(R_data)\n",
    "        self.D = torch.tensor(D_data)\n",
    "        self.losses = [] # here I saved the model's losses per epoch\n",
    "\n",
    "        #setting the parameters\n",
    "        self.L_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.kappa_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.q_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.sigma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.gamma_vax_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.delta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #find values for normalization\n",
    "        self.S_max = max(self.S)\n",
    "        self.E_max = max(self.E)\n",
    "        self.V_max = max(self.V)\n",
    "        self.I_max = max(self.I)\n",
    "        self.R_max = max(self.R)\n",
    "        self.D_max = max(self.D)\n",
    "\n",
    "        self.S_min = min(self.S)\n",
    "        self.E_min = min(self.E)\n",
    "        self.V_min = min(self.V)\n",
    "        self.I_min = min(self.I)\n",
    "        self.R_min = min(self.R)\n",
    "        self.D_min = min(self.D)\n",
    "\n",
    "        #normalize\n",
    "        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
    "        self.E_hat = (self.E - self.E_min) / (self.E_max - self.E_min)\n",
    "        self.V_hat = (self.V - self.V_min) / (self.V_max - self.V_min)\n",
    "        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
    "        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)        \n",
    "        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
    "\n",
    "        #matrices (x6 for S,E,V,I,R,D) for the gradients\n",
    "        self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1\n",
    "        self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1\n",
    "        self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1\n",
    "        self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1\n",
    "        self.m5 = torch.zeros((len(self.t), 4)); self.m5[:, 4] = 1\n",
    "        self.m6 = torch.zeros((len(self.t), 4)); self.m6[:, 5] = 1\n",
    "        \n",
    "        #NN\n",
    "        self.net_sidr = self.Net_sidr()\n",
    "        self.params = list(self.net_sidr.parameters())\n",
    "        self.params.extend(\n",
    "            list(\n",
    "                [\n",
    "                    self.L_tilda,\n",
    "                    self.beta_tilda,\n",
    "                    self.kappa_tilda,\n",
    "                    self.alpha_tilda,\n",
    "                    self.q_tilda,\n",
    "                    self.sigma_tilda,\n",
    "                    self.gamma_tilda,\n",
    "                    self.gamma_vax_tilda,\n",
    "                    self.delta_tilda,\n",
    "                    self.mu_tilda\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "########################        \n",
    "#### HERE  ####\n",
    "################\n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return torch.tanh(self.alpha_tilda) #* 0.1 + 0.2\n",
    "\n",
    "    @property\n",
    "    def beta(self):\n",
    "        return torch.tanh(self.beta_tilda) #* 0.01 + 0.05\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return torch.tanh(self.gamma_tilda) #* 0.01 + 0.03\n",
    "\n",
    "    class Net_sidr(nn.Module): # input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps \n",
    "        def __init__(self):\n",
    "            super(DINN.Net_sidr, self).__init__()\n",
    "\n",
    "            self.fc1=nn.Linear(1, 20) #takes 100 t's\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.fc3=nn.Linear(20, 20)\n",
    "            self.fc4=nn.Linear(20, 20)\n",
    "            self.fc5=nn.Linear(20, 20)\n",
    "            self.fc6=nn.Linear(20, 20)\n",
    "            self.fc7=nn.Linear(20, 20)\n",
    "            self.fc8=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 4) #outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)\n",
    "\n",
    "        def forward(self, t_batch):\n",
    "            sidr=F.relu(self.fc1(t_batch))\n",
    "            sidr=F.relu(self.fc2(sidr))\n",
    "            sidr=F.relu(self.fc3(sidr))\n",
    "            sidr=F.relu(self.fc4(sidr))\n",
    "            sidr=F.relu(self.fc5(sidr))\n",
    "            sidr=F.relu(self.fc6(sidr))\n",
    "            sidr=F.relu(self.fc7(sidr))\n",
    "            sidr=F.relu(self.fc8(sidr))\n",
    "            sidr=self.out(sidr)\n",
    "            return sidr\n",
    "\n",
    "    def net_f(self, t_batch):\n",
    "            \n",
    "            #pass the timesteps batch to the neural network\n",
    "            sidr_hat = self.net_sidr(t_batch)\n",
    "            \n",
    "            #organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the \"hat\" part\n",
    "            S_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]\n",
    "\n",
    "            #S_t\n",
    "            sidr_hat.backward(self.m1, retain_graph=True)\n",
    "            S_hat_t = self.t.grad.clone()\n",
    "            self.t.grad.zero_()\n",
    "\n",
    "            #I_t\n",
    "            sidr_hat.backward(self.m2, retain_graph=True)\n",
    "            I_hat_t = self.t.grad.clone()\n",
    "            self.t.grad.zero_()\n",
    "\n",
    "            #D_t\n",
    "            sidr_hat.backward(self.m3, retain_graph=True)\n",
    "            D_hat_t = self.t.grad.clone()\n",
    "            self.t.grad.zero_()\n",
    "\n",
    "            #R_t\n",
    "            sidr_hat.backward(self.m4, retain_graph=True)\n",
    "            R_hat_t = self.t.grad.clone()\n",
    "            self.t.grad.zero_() \n",
    "\n",
    "            #unnormalize\n",
    "            S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
    "            I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
    "            D = self.D_min + (self.D_max - self.D_min) * D_hat      \n",
    "            R = self.R_min + (self.R_max - self.R_min) * R_hat        \n",
    "\n",
    "            f1_hat = S_hat_t - (-(self.alpha / self.N) * S * I)  / (self.S_max - self.S_min)\n",
    "            f2_hat = I_hat_t - ((self.alpha / self.N) * S * I - self.beta * I - self.gamma * I ) / (self.I_max - self.I_min)\n",
    "            f3_hat = D_hat_t - (self.gamma * I) / (self.D_max - self.D_min)\n",
    "            f4_hat = R_hat_t - (self.beta * I ) / (self.R_max - self.R_min)        \n",
    "\n",
    "            return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        # train\n",
    "        print('\\nstarting training...\\n')\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # lists to hold the output (maintain only the final epoch)\n",
    "            S_pred_list = []\n",
    "            I_pred_list = []\n",
    "            D_pred_list = []\n",
    "            R_pred_list = []\n",
    "\n",
    "            # we pass the timesteps batch into net_f\n",
    "            f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch) # net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n",
    "            \n",
    "            self.optimizer.zero_grad() #zero grad\n",
    "            \n",
    "            #append the values to plot later (note that we unnormalize them here for plotting)\n",
    "            S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n",
    "            I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n",
    "            D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)\n",
    "            R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n",
    "\n",
    "            #calculate the loss --- MSE of the neural networks output and each compartment\n",
    "            loss = (torch.mean(torch.square(self.S_hat - S_pred))+ \n",
    "                    torch.mean(torch.square(self.I_hat - I_pred))+\n",
    "                    torch.mean(torch.square(self.D_hat - D_pred))+\n",
    "                    torch.mean(torch.square(self.R_hat - R_pred))+\n",
    "                    torch.mean(torch.square(f1))+\n",
    "                    torch.mean(torch.square(f2))+\n",
    "                    torch.mean(torch.square(f3))+\n",
    "                    torch.mean(torch.square(f4))\n",
    "                    ) \n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step() \n",
    "\n",
    "            # append the loss value (we call \"loss.item()\" because we just want the value of the loss and not the entire computational graph)\n",
    "            self.losses.append(loss.item())\n",
    "\n",
    "            if epoch % 1000 == 0:          \n",
    "                print('\\nEpoch ', epoch)\n",
    "                print('alpha: (goal 0.191 ', self.alpha)\n",
    "                print('beta: (goal 0.05 ', self.beta)\n",
    "                print('gamma: (goal 0.0294 ', self.gamma)\n",
    "                print('#################################')                \n",
    "\n",
    "        return S_pred_list, I_pred_list, D_pred_list, R_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c0c9d-f995-49ac-83ff-e93d5d8f1d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dinn = DINN(\n",
    "    data[0],\n",
    "    data[1],\n",
    "    data[2],\n",
    "    data[3], \n",
    "    data[4]\n",
    ") #in the form of [t,S,I,D,R]\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = optim.Adam(dinn.params, lr = learning_rate)\n",
    "dinn.optimizer = optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    dinn.optimizer,\n",
    "    base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "\n",
    "dinn.scheduler = scheduler\n",
    "\n",
    "S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(50000) #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbd608-7c67-41b2-b477-2edd094010ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dinn.losses[0:], color = 'teal')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ba1a9-b156-4389-80c7-9deeeceef182",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "ax.plot(data[0], data[1], 'pink', alpha=1, lw=2, label='Susceptible')\n",
    "ax.scatter(data[0], S_pred_list[0].detach().numpy(), color='red', marker=\".\", alpha=1 , label='Susceptible Prediction')\n",
    "\n",
    "ax.plot(data[0], data[2], 'violet', alpha=1, lw=2, label='Infected')\n",
    "ax.scatter(data[0], I_pred_list[0].detach().numpy(), color='dodgerblue', alpha=1, marker=\".\", label='Infected Prediction')\n",
    "\n",
    "ax.plot(data[0], data[3], 'darkgreen', alpha=0.5, lw=2, label='Dead')\n",
    "ax.scatter(data[0], D_pred_list[0].detach().numpy(), color='green', alpha=1, marker=\".\", label='Dead Prediction')\n",
    "\n",
    "ax.plot(data[0], data[4], 'blue', alpha=0.5, lw=2, label='Recovered')\n",
    "ax.scatter(data[0], R_pred_list[0].detach().numpy(), color='teal', alpha=1, marker=\".\", label='Recovered Prediction')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Time /days')\n",
    "ax.set_ylabel('Number')\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.xaxis.set_tick_params(length=0)\n",
    "ax.grid(visible=True, which='major', c='black', lw=0.2, ls='-')\n",
    "legend = ax.legend()\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "for spine in ('top', 'right', 'bottom', 'left'):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
